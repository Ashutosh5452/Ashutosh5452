{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Hi, Akshit here!\n",
        "# Credits for this goes to Sujay!\n",
        "\n",
        "scored 9/10 in phase A.\n",
        "\n",
        "Submit Your Project here: https://docs.google.com/forms/d/e/1FAIpQLSdOaljgV-INdbKrPotV9OMUKV01QVaFEfcnr5dAxBZqM4x37g/viewform"
      ],
      "metadata": {
        "id": "LlaAIKuOrJ7C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-requisites for Project:\n",
        "\n",
        "Go through this video for first 2 hrs, it basically covers the wsl, git, github, docker related stuff.\n",
        "```\n",
        "https://www.youtube.com/watch?v=jXj6bqy4R4c&list=PL_h5u1jMeBCl1BquBhgunA4t08XAxsA-C&index=17\n",
        "```\n",
        "\n",
        "```\n",
        "The above video contains:\n",
        "1. Creating a new public git repo.\n",
        "2. Adding MIT lincense to the repo.\n",
        "3. Commiting and pushing the code.\n",
        "4. Creating a docker file that builds our application.\n",
        "5. Publishing our docker image publicly to docker hub.\n",
        "```"
      ],
      "metadata": {
        "id": "PQ-cE2l8rcCe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A1 Task: Install uv and Run datagen.py\n",
        "What this task does:\n",
        "- Installs uv (if not installed).\n",
        "- Runs a script from a URL (datagen.py) with your email as an argument.\n",
        "- Generates data files required for later tasks.\n",
        "\n",
        "-----\n",
        "## Task A2: Format Markdown Using Prettier\n",
        "What this task does:\n",
        "- Takes a Markdown file (/data/format.md).\n",
        "- Formats it in-place using prettier@3.4.2.\n",
        "- Uses npx (Node.js package runner) to execute Prettier.\n",
        "---\n",
        "## Task A3: Count the Number of Wednesdays in /data/dates.txt\n",
        "\n",
        "What this task does:\n",
        "- Reads the file /data/dates.txt, which contains a list of dates (one per line).\n",
        "- Counts how many dates are Wednesdays.\n",
        "- Writes the result to /data/dates-wednesdays.txt (only the number).\n",
        "\n",
        "---\n",
        "## Task A4: Sort Contacts in /data/contacts.json\n",
        "What this task does:\n",
        "\n",
        "- Reads the file /data/contacts.json, which contains a list of contacts.\n",
        "- Sorts the contacts by last_name, then first_name.\n",
        "- Writes the sorted list to /data/contacts-sorted.json.\n",
        "---\n",
        "## Task A5: Extract First Lines from the 10 Most Recent .log Files\n",
        "What this task does:\n",
        "\n",
        "- Finds all .log files inside /data/logs/.\n",
        "- Sorts them by modification time (newest first).\n",
        "- Extracts the first line from the 10 most recent log files.\n",
        "- Writes the result to /data/logs-recent.txt (one line per log file).\n",
        "---\n",
        "## Task A6: Extract Titles from Markdown (.md) Files\n",
        "What this task does:\n",
        "\n",
        "- Scans all Markdown (.md) files in /data/docs/.\n",
        "- Extracts the first H1 title (i.e., a line starting with # ).\n",
        "- Creates an index file (/data/docs/index.json) that maps filenames to their titles.\n",
        "---\n",
        "## Task A7: Extract the Sender’s Email from /data/email.txt Using LLM\n",
        "What this task does:\n",
        "\n",
        "- Reads the email content from /data/email.txt.\n",
        "- Uses GPT-4o-Mini (via AI Proxy API) to extract the sender’s email.\n",
        "- Writes the extracted email to /data/email-sender.txt.\n",
        "---\n",
        "##  Task A8: Extract a Credit Card Number from an Image Using LLM\n",
        "What this task does:\n",
        "\n",
        "- Reads the image file /data/credit-card.png.\n",
        "- Uses GPT-4o-Mini (via AI Proxy API) to extract the credit card number.\n",
        "- Writes the extracted number (without spaces) to /data/credit-card.txt.\n",
        "---\n",
        "##  Task A9: Find the Most Similar Pair of Comments Using Embeddings\n",
        "What this task does:\n",
        "\n",
        "- Reads /data/comments.txt, which contains a list of comments (one per line).\n",
        "- Uses text embeddings (via AI Proxy API, text-embedding-3-small) to find the most similar pair.\n",
        "- Writes the two most similar comments to /data/comments-similar.txt (one per line).\n",
        "---\n",
        "## Task A10: Calculate Total Sales for \"Gold\" Tickets in an SQLite Database\n",
        "What this task does:\n",
        "\n",
        "- Reads the SQLite database file /data/ticket-sales.db.\n",
        "- Queries the tickets table to sum the total units * price for \"Gold\" tickets.\n",
        "- Writes the result to /data/ticket-sales-gold.txt."
      ],
      "metadata": {
        "id": "vI6FX1FlttIu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "---\n",
        "# PROCESS\n",
        "1. Create a folder and add below files in it. (app.py, dockerfile, evaluate.py, datagen.py, tasksA.py, tasksB.py)\n",
        "2. Create a file called .env and add\n",
        "```\n",
        "AIPROXY_TOKEN= (your token without quotes, paste it here as it is)\n",
        "```\n",
        "3. Open terminal and type: ``uv run app.py``\n",
        "4. Open new terminal simultaneously and type: ``uv run evaluate.py --email=xxxxxxxxx@ds.study.iitm.ac.in --log-level=INFO``\n",
        "5. Make sure you have uv and npx installed."
      ],
      "metadata": {
        "id": "qZb-STubaRo8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# app.py"
      ],
      "metadata": {
        "id": "aWrz_czBbyf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# app.py\n",
        "# /// script\n",
        "# dependencies = [\n",
        "#   \"requests\",\n",
        "#   \"fastapi\",\n",
        "#   \"uvicorn\",\n",
        "#   \"python-dateutil\",\n",
        "#   \"pandas\",\n",
        "#   \"db-sqlite3\",\n",
        "#   \"scipy\",\n",
        "#   \"pybase64\",\n",
        "#   \"python-dotenv\",\n",
        "#   \"httpx\",\n",
        "#   \"markdown\",\n",
        "#   \"duckdb\"\n",
        "# ]\n",
        "# ///\n",
        "\n",
        "from fastapi import FastAPI, HTTPException, Query\n",
        "from fastapi.responses import PlainTextResponse, JSONResponse\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from tasksA import *\n",
        "from tasksB import *\n",
        "import requests\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import re\n",
        "import httpx\n",
        "import json\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"GET\", \"POST\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "\n",
        "app = FastAPI()\n",
        "load_dotenv()\n",
        "\n",
        "# @app.get('/ask')\n",
        "# def ask(prompt: str):\n",
        "#     \"\"\" Prompt Gemini to generate a response based on the given prompt. \"\"\"\n",
        "#     gemini_api_key = os.getenv('gemini_api_key')\n",
        "#     if not gemini_api_key:\n",
        "#         return JSONResponse(content={\"error\": \"GEMINI_API_KEY not set\"}, status_code=500)\n",
        "\n",
        "#     # Read the contents of tasks.py\n",
        "#     with open('tasks.py', 'r') as file:\n",
        "#         tasks_content = file.read()\n",
        "\n",
        "#     # Prepare the request data\n",
        "#     data = {\n",
        "#         \"contents\": [{\n",
        "#             \"parts\": [\n",
        "#                 {\"text\": f\"Find the task function from here for the below prompt:\\n{tasks_content}\\n\\nPrompt: {prompt}\\n\\n respond with the function_name and function_parameters with parameters in json format\"},\n",
        "#             ]\n",
        "#         }]\n",
        "#     }\n",
        "\n",
        "#     url = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={gemini_api_key}\"\n",
        "#     headers = {\n",
        "#         \"Content-Type\": \"application/json\"\n",
        "#     }\n",
        "\n",
        "#     response = requests.post(url, json=data, headers=headers)\n",
        "\n",
        "#     if response.status_code == 200:\n",
        "#         text_reponse = response.json()[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
        "#         match = re.search(r'```json\\n(.*?)\\n```', text_reponse, re.DOTALL)\n",
        "#         text_reponse = match.group(1).strip() if match else text_reponse\n",
        "#         return json.loads(text_reponse)\n",
        "#         # return JSONResponse(content=response.json(), status_code=200)\n",
        "#     else:\n",
        "#         return JSONResponse(content={\"error\": \"Failed to get response\", \"details\": response.text}, status_code=response.status_code)\n",
        "\n",
        "@app.get(\"/ask\")\n",
        "def ask(prompt: str):\n",
        "    result = get_completions(prompt)\n",
        "    return result\n",
        "\n",
        "openai_api_chat  = \"http://aiproxy.sanand.workers.dev/openai/v1/chat/completions\" # for testing\n",
        "openai_api_key = os.getenv(\"AIPROXY_TOKEN\")\n",
        "\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {openai_api_key}\",\n",
        "    \"Content-Type\": \"application/json\",\n",
        "}\n",
        "\n",
        "function_definitions_llm = [\n",
        "    {\n",
        "        \"name\": \"A1\",\n",
        "        \"description\": \"Run a Python script from a given URL, passing an email as the argument.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                # \"filename\": {\"type\": \"string\", \"pattern\": r\"https?://.*\\.py\"},\n",
        "                # \"targetfile\": {\"type\": \"string\", \"pattern\": r\".*/(.*\\.py)\"},\n",
        "                \"email\": {\"type\": \"string\", \"pattern\": r\"[\\w\\.-]+@[\\w\\.-]+\\.\\w+\"}\n",
        "            },\n",
        "            \"required\": [\"filename\", \"targetfile\", \"email\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"A2\",\n",
        "        \"description\": \"Format a markdown file using a specified version of Prettier.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"prettier_version\": {\"type\": \"string\", \"pattern\": r\"prettier@\\d+\\.\\d+\\.\\d+\"},\n",
        "                \"filename\": {\"type\": \"string\", \"pattern\": r\".*/(.*\\.md)\"}\n",
        "            },\n",
        "            \"required\": [\"prettier_version\", \"filename\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"A3\",\n",
        "        \"description\": \"Count the number of occurrences of a specific weekday in a date file.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"filename\": {\"type\": \"string\", \"pattern\": r\"/data/.*dates.*\\.txt\"},\n",
        "                \"targetfile\": {\"type\": \"string\", \"pattern\": r\"/data/.*/(.*\\.txt)\"},\n",
        "                \"weekday\": {\"type\": \"integer\", \"pattern\": r\"(Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday)\"}\n",
        "            },\n",
        "            \"required\": [\"filename\", \"targetfile\", \"weekday\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"A4\",\n",
        "        \"description\": \"Sort a JSON contacts file and save the sorted version to a target file.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"filename\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"pattern\": r\".*/(.*\\.json)\",\n",
        "                },\n",
        "                \"targetfile\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"pattern\": r\".*/(.*\\.json)\",\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"filename\", \"targetfile\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"A5\",\n",
        "        \"description\": \"Retrieve the most recent log files from a directory and save their content to an output file.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"log_dir_path\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"pattern\": r\".*/logs\",\n",
        "                    \"default\": \"/data/logs\"\n",
        "                },\n",
        "                \"output_file_path\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"pattern\": r\".*/(.*\\.txt)\",\n",
        "                    \"default\": \"/data/logs-recent.txt\"\n",
        "                },\n",
        "                \"num_files\": {\n",
        "                    \"type\": \"integer\",\n",
        "                    \"minimum\": 1,\n",
        "                    \"default\": 10\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"log_dir_path\", \"output_file_path\", \"num_files\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"A6\",\n",
        "        \"description\": \"Generate an index of documents from a directory and save it as a JSON file.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"doc_dir_path\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"pattern\": r\".*/docs\",\n",
        "                    \"default\": \"/data/docs\"\n",
        "                },\n",
        "                \"output_file_path\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"pattern\": r\".*/(.*\\.json)\",\n",
        "                    \"default\": \"/data/docs/index.json\"\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"doc_dir_path\", \"output_file_path\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"A7\",\n",
        "        \"description\": \"Extract the sender's email address from a text file and save it to an output file.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"filename\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"pattern\": r\".*/(.*\\.txt)\",\n",
        "                    \"default\": \"/data/email.txt\"\n",
        "                },\n",
        "                \"output_file\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"pattern\": r\".*/(.*\\.txt)\",\n",
        "                    \"default\": \"/data/email-sender.txt\"\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"filename\", \"output_file\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"A8\",\n",
        "        \"description\": \"Generate an image representation of credit card details from a text file.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"filename\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"pattern\": r\".*/(.*\\.txt)\",\n",
        "                    \"default\": \"/data/credit-card.txt\"\n",
        "                },\n",
        "                \"image_path\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"pattern\": r\".*/(.*\\.png)\",\n",
        "                    \"default\": \"/data/credit-card.png\"\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"filename\", \"image_path\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"A9\",\n",
        "        \"description\": \"Find similar comments from a text file and save them to an output file.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"filename\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"pattern\": r\".*/(.*\\.txt)\",\n",
        "                    \"default\": \"/data/comments.txt\"\n",
        "                },\n",
        "                \"output_filename\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"pattern\": r\".*/(.*\\.txt)\",\n",
        "                    \"default\": \"/data/comments-similar.txt\"\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"filename\", \"output_filename\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"A10\",\n",
        "        \"description\": \"Identify high-value (gold) ticket sales from a database and save them to a text file.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"filename\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"pattern\": r\".*/(.*\\.db)\",\n",
        "                    \"default\": \"/data/ticket-sales.db\"\n",
        "                },\n",
        "                \"output_filename\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"pattern\": r\".*/(.*\\.txt)\",\n",
        "                    \"default\": \"/data/ticket-sales-gold.txt\"\n",
        "                },\n",
        "                \"query\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"pattern\": \"SELECT SUM(units * price) FROM tickets WHERE type = 'Gold'\"\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"filename\", \"output_filename\", \"query\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"B12\",\n",
        "        \"description\": \"Check if filepath starts with /data\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"filepath\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"pattern\": r\"^/data/.*\",\n",
        "                    # \"description\": \"Filepath must start with /data to ensure secure access.\"\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"filepath\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"B3\",\n",
        "        \"description\": \"Download content from a URL and save it to the specified path.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"url\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"pattern\": r\"https?://.*\",\n",
        "                    \"description\": \"URL to download content from.\"\n",
        "                },\n",
        "                \"save_path\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"pattern\": r\".*/.*\",\n",
        "                    \"description\": \"Path to save the downloaded content.\"\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"url\", \"save_path\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"B5\",\n",
        "        \"description\": \"Execute a SQL query on a specified database file and save the result to an output file.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"db_path\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"pattern\": r\".*/(.*\\.db)\",\n",
        "                    \"description\": \"Path to the SQLite database file.\"\n",
        "                },\n",
        "                \"query\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"SQL query to be executed on the database.\"\n",
        "                },\n",
        "                \"output_filename\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"pattern\": r\".*/(.*\\.txt)\",\n",
        "                    \"description\": \"Path to the file where the query result will be saved.\"\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"db_path\", \"query\", \"output_filename\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"B6\",\n",
        "        \"description\": \"Fetch content from a URL and save it to the specified output file.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"url\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"pattern\": r\"https?://.*\",\n",
        "                    \"description\": \"URL to fetch content from.\"\n",
        "                },\n",
        "                \"output_filename\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"pattern\": r\".*/.*\",\n",
        "                    \"description\": \"Path to the file where the content will be saved.\"\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"url\", \"output_filename\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"B7\",\n",
        "        \"description\": \"Process an image by optionally resizing it and saving the result to an output path.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"image_path\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"pattern\": r\".*/(.*\\.(jpg|jpeg|png|gif|bmp))\",\n",
        "                    \"description\": \"Path to the input image file.\"\n",
        "                },\n",
        "                \"output_path\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"pattern\": r\".*/.*\",\n",
        "                    \"description\": \"Path to save the processed image.\"\n",
        "                },\n",
        "                \"resize\": {\n",
        "                    \"type\": \"array\",\n",
        "                    \"items\": {\n",
        "                        \"type\": \"integer\",\n",
        "                        \"minimum\": 1\n",
        "                    },\n",
        "                    \"minItems\": 2,\n",
        "                    \"maxItems\": 2,\n",
        "                    \"description\": \"Optional. Resize dimensions as [width, height].\"\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"image_path\", \"output_path\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"B9\",\n",
        "        \"description\": \"Convert a Markdown file to another format and save the result to the specified output path.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"md_path\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"pattern\": r\".*/(.*\\.md)\",\n",
        "                    \"description\": \"Path to the Markdown file to be converted.\"\n",
        "                },\n",
        "                \"output_path\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"pattern\": r\".*/.*\",\n",
        "                    \"description\": \"Path where the converted file will be saved.\"\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"md_path\", \"output_path\"]\n",
        "        }\n",
        "    }\n",
        "\n",
        "]\n",
        "\n",
        "def get_completions(prompt: str):\n",
        "    with httpx.Client(timeout=20) as client:\n",
        "        response = client.post(\n",
        "            f\"{openai_api_chat}\",\n",
        "            headers=headers,\n",
        "            json=\n",
        "                {\n",
        "                    \"model\": \"gpt-4o-mini\",\n",
        "                    \"messages\": [\n",
        "                                    {\"role\": \"system\", \"content\": \"You are a function classifier that extracts structured parameters from queries.\"},\n",
        "                                    {\"role\": \"user\", \"content\": prompt}\n",
        "                                ],\n",
        "                    \"tools\": [\n",
        "                                {\n",
        "                                    \"type\": \"function\",\n",
        "                                    \"function\": function\n",
        "                                } for function in function_definitions_llm\n",
        "                            ],\n",
        "                    \"tool_choice\": \"auto\"\n",
        "                },\n",
        "        )\n",
        "    # return response.json()\n",
        "    print(response.json()[\"choices\"][0][\"message\"][\"tool_calls\"][0][\"function\"])\n",
        "    return response.json()[\"choices\"][0][\"message\"][\"tool_calls\"][0][\"function\"]\n",
        "\n",
        "\n",
        "# Placeholder for task execution\n",
        "@app.post(\"/run\")\n",
        "async def run_task(task: str):\n",
        "    try:\n",
        "        # Placeholder logic for executing tasks\n",
        "        # Replace with actual logic to parse task and execute steps\n",
        "        # Example: Execute task and return success or error based on result\n",
        "        # llm_response = function_calling(tast), function_name = A1\n",
        "        response = get_completions(task)\n",
        "        print(response)\n",
        "        task_code = response['name']\n",
        "        arguments = response['arguments']\n",
        "\n",
        "        if \"A1\"== task_code:\n",
        "            A1(**json.loads(arguments))\n",
        "        if \"A2\"== task_code:\n",
        "            A2(**json.loads(arguments))\n",
        "        if \"A3\"== task_code:\n",
        "            A3(**json.loads(arguments))\n",
        "        if \"A4\"== task_code:\n",
        "            A4(**json.loads(arguments))\n",
        "        if \"A5\"== task_code:\n",
        "            A5(**json.loads(arguments))\n",
        "        if \"A6\"== task_code:\n",
        "            A6(**json.loads(arguments))\n",
        "        if \"A7\"== task_code:\n",
        "            A7(**json.loads(arguments))\n",
        "        if \"A8\"== task_code:\n",
        "            A8(**json.loads(arguments))\n",
        "        if \"A9\"== task_code:\n",
        "            A9(**json.loads(arguments))\n",
        "        if \"A10\"== task_code:\n",
        "            A10(**json.loads(arguments))\n",
        "\n",
        "\n",
        "        if \"B12\"== task_code:\n",
        "            B12(**json.loads(arguments))\n",
        "        if \"B3\" == task_code:\n",
        "            B3(**json.loads(arguments))\n",
        "        if \"B5\" == task_code:\n",
        "            B5(**json.loads(arguments))\n",
        "        if \"B6\" == task_code:\n",
        "            B6(**json.loads(arguments))\n",
        "        if \"B7\" == task_code:\n",
        "            B7(**json.loads(arguments))\n",
        "        if \"B9\" == task_code:\n",
        "            B9(**json.loads(arguments))\n",
        "        return {\"message\": f\"{task_code} Task '{task}' executed successfully\"}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=400, detail=str(e))\n",
        "\n",
        "# Placeholder for file reading\n",
        "@app.get(\"/read\", response_class=PlainTextResponse)\n",
        "async def read_file(path: str = Query(..., description=\"File path to read\")):\n",
        "    try:\n",
        "        with open(path, \"r\") as file:\n",
        "            return file.read()\n",
        "    except FileNotFoundError:\n",
        "        raise HTTPException(status_code=404, detail=\"File not found\")\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import uvicorn\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
      ],
      "metadata": {
        "id": "-wiy0ZgzauCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# tasksA.py\n",
        "(change your mail id)"
      ],
      "metadata": {
        "id": "HwN_EmYWb1AM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import subprocess\n",
        "from dateutil.parser import parse\n",
        "from datetime import datetime\n",
        "import json\n",
        "from pathlib import Path\n",
        "import os\n",
        "import requests\n",
        "from scipy.spatial.distance import cosine\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "AIPROXY_TOKEN = os.getenv('AIPROXY_TOKEN')\n",
        "\n",
        "\n",
        "def A1(email=\"xxxxxxxxx@ds.study.iitm.ac.in\"):\n",
        "    try:\n",
        "        process = subprocess.Popen(\n",
        "            [\"uv\", \"run\", \"https://raw.githubusercontent.com/sanand0/tools-in-data-science-public/tds-2025-01/project-1/datagen.py\", email],\n",
        "            stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True\n",
        "        )\n",
        "        stdout, stderr = process.communicate()\n",
        "        if process.returncode != 0:\n",
        "            raise HTTPException(status_code=500, detail=f\"Error: {stderr}\")\n",
        "        return stdout\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        raise HTTPException(status_code=500, detail=f\"Error: {e.stderr}\")\n",
        "# A1()\n",
        "def A2(prettier_version=\"prettier@3.4.2\", filename=\"/data/format.md\"):\n",
        "    command = [r\"C:\\Program Files\\nodejs\\npx.cmd\", prettier_version, \"--write\", filename]\n",
        "    try:\n",
        "        subprocess.run(command, check=True)\n",
        "        print(\"Prettier executed successfully.\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "def A3(filename='/data/dates.txt', targetfile='/data/dates-wednesdays.txt', weekday=2):\n",
        "    input_file = filename\n",
        "    output_file = targetfile\n",
        "    weekday = weekday\n",
        "    weekday_count = 0\n",
        "\n",
        "    with open(input_file, 'r') as file:\n",
        "        weekday_count = sum(1 for date in file if parse(date).weekday() == int(weekday)-1)\n",
        "\n",
        "\n",
        "    with open(output_file, 'w') as file:\n",
        "        file.write(str(weekday_count))\n",
        "\n",
        "def A4(filename=\"/data/contacts.json\", targetfile=\"/data/contacts-sorted.json\"):\n",
        "    # Load the contacts from the JSON file\n",
        "    with open(filename, 'r') as file:\n",
        "        contacts = json.load(file)\n",
        "\n",
        "    # Sort the contacts by last_name and then by first_name\n",
        "    sorted_contacts = sorted(contacts, key=lambda x: (x['last_name'], x['first_name']))\n",
        "\n",
        "    # Write the sorted contacts to the new JSON file\n",
        "    with open(targetfile, 'w') as file:\n",
        "        json.dump(sorted_contacts, file, indent=4)\n",
        "\n",
        "def A5(log_dir_path='/data/logs', output_file_path='/data/logs-recent.txt', num_files=10):\n",
        "    log_dir = Path(log_dir_path)\n",
        "    output_file = Path(output_file_path)\n",
        "\n",
        "    # Get list of .log files sorted by modification time (most recent first)\n",
        "    log_files = sorted(log_dir.glob('*.log'), key=os.path.getmtime, reverse=True)[:num_files]\n",
        "\n",
        "    # Read first line of each file and write to the output file\n",
        "    with output_file.open('w') as f_out:\n",
        "        for log_file in log_files:\n",
        "            with log_file.open('r') as f_in:\n",
        "                first_line = f_in.readline().strip()\n",
        "                f_out.write(f\"{first_line}\\n\")\n",
        "\n",
        "def A6(doc_dir_path='/data/docs', output_file_path='/data/docs/index.json'):\n",
        "    docs_dir = doc_dir_path\n",
        "    output_file = output_file_path\n",
        "    index_data = {}\n",
        "\n",
        "    # Walk through all files in the docs directory\n",
        "    for root, _, files in os.walk(docs_dir):\n",
        "        for file in files:\n",
        "            if file.endswith('.md'):\n",
        "                # print(file)\n",
        "                file_path = os.path.join(root, file)\n",
        "                # Read the file and find the first occurrence of an H1\n",
        "                with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                    for line in f:\n",
        "                        if line.startswith('# '):\n",
        "                            # Extract the title text after '# '\n",
        "                            title = line[2:].strip()\n",
        "                            # Get the relative path without the prefix\n",
        "                            relative_path = os.path.relpath(file_path, docs_dir).replace('\\\\', '/')\n",
        "                            index_data[relative_path] = title\n",
        "                            break  # Stop after the first H1\n",
        "    # Write the index data to index.json\n",
        "    # print(index_data)\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(index_data, f, indent=4)\n",
        "\n",
        "def A7(filename='/data/email.txt', output_file='/data/email-sender.txt'):\n",
        "    # Read the content of the email\n",
        "    with open(filename, 'r') as file:\n",
        "        email_content = file.readlines()\n",
        "\n",
        "    sender_email = \"sujay@gmail.com\"\n",
        "    for line in email_content:\n",
        "        if \"From\" == line[:4]:\n",
        "            sender_email = (line.strip().split(\" \")[-1]).replace(\"<\", \"\").replace(\">\", \"\")\n",
        "            break\n",
        "\n",
        "    # Get the extracted email address\n",
        "\n",
        "    # Write the email address to the output file\n",
        "    with open(output_file, 'w') as file:\n",
        "        file.write(sender_email)\n",
        "\n",
        "import base64\n",
        "def png_to_base64(image_path):\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "        base64_string = base64.b64encode(image_file.read()).decode('utf-8')\n",
        "    return base64_string\n",
        "# def A8():\n",
        "#     input_image = \"data/credit_card.png\"\n",
        "#     output_file = \"data/credit-card.txt\"\n",
        "\n",
        "#     # Step 1: Extract text using OCR\n",
        "#     try:\n",
        "#         image = Image.open(input_image)\n",
        "#         extracted_text = pytesseract.image_to_string(image)\n",
        "#         print(f\"Extracted text:\\n{extracted_text}\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"❌ Error reading or processing {input_image}: {e}\")\n",
        "#         return\n",
        "\n",
        "#     # Step 2: Pass the extracted text to the LLM to validate and extract card number\n",
        "#     prompt = f\"\"\"Extract the credit card number from the following text. Respond with only the card number, without spaces:\n",
        "\n",
        "#     {extracted_text}\n",
        "#     \"\"\"\n",
        "#     try:\n",
        "#         card_number = ask_llm(prompt).strip()\n",
        "#         print(f\"Card number extracted by LLM: {card_number}\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"❌ Error processing with LLM: {e}\")\n",
        "#         return\n",
        "\n",
        "#     # Step 3: Save the extracted card number to a text file\n",
        "#     try:\n",
        "#         with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
        "#             file.write(card_number + \"\\n\")\n",
        "#         print(f\"✅ Credit card number saved to: {output_file}\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"❌ Error writing {output_file}: {e}\")\n",
        "\n",
        "def A8(filename='/data/credit_card.txt', image_path='/data/credit_card.png'):\n",
        "    # Construct the request body for the AIProxy call\n",
        "    body = {\n",
        "        \"model\": \"gpt-4o-mini\",\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\n",
        "                        \"type\": \"text\",\n",
        "                        \"text\": \"There is 8 or more digit number is there in this image, with space after every 4 digit, only extract the those digit number without spaces and return just the number without any other characters\"\n",
        "                    },\n",
        "                    {\n",
        "                        \"type\": \"image_url\",\n",
        "                        \"image_url\": {\n",
        "                            \"url\": f\"data:image/png;base64,{png_to_base64(image_path)}\"\n",
        "                        }\n",
        "                    }\n",
        "                ]\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": f\"Bearer {AIPROXY_TOKEN}\"\n",
        "    }\n",
        "\n",
        "    # Make the request to the AIProxy service\n",
        "    response = requests.post(\"http://aiproxy.sanand.workers.dev/openai/v1/chat/completions\",\n",
        "                             headers=headers, data=json.dumps(body))\n",
        "    # response.raise_for_status()\n",
        "\n",
        "    # Extract the credit card number from the response\n",
        "    result = response.json()\n",
        "    # print(result); return None\n",
        "    card_number = result['choices'][0]['message']['content'].replace(\" \", \"\")\n",
        "\n",
        "    # Write the extracted card number to the output file\n",
        "    with open(filename, 'w') as file:\n",
        "        file.write(card_number)\n",
        "# A8()\n",
        "\n",
        "\n",
        "\n",
        "def get_embedding(text):\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": f\"Bearer {AIPROXY_TOKEN}\"\n",
        "    }\n",
        "    data = {\n",
        "        \"model\": \"text-embedding-3-small\",\n",
        "        \"input\": [text]\n",
        "    }\n",
        "    response = requests.post(\"http://aiproxy.sanand.workers.dev/openai/v1/embeddings\", headers=headers, data=json.dumps(data))\n",
        "    response.raise_for_status()\n",
        "    return response.json()[\"data\"][0][\"embedding\"]\n",
        "\n",
        "def A9(filename='/data/comments.txt', output_filename='/data/comments-similar.txt'):\n",
        "    # Read comments\n",
        "    with open(filename, 'r') as f:\n",
        "        comments = [line.strip() for line in f.readlines()]\n",
        "\n",
        "    # Get embeddings for all comments\n",
        "    embeddings = [get_embedding(comment) for comment in comments]\n",
        "\n",
        "    # Find the most similar pair\n",
        "    min_distance = float('inf')\n",
        "    most_similar = (None, None)\n",
        "\n",
        "    for i in range(len(comments)):\n",
        "        for j in range(i + 1, len(comments)):\n",
        "            distance = cosine(embeddings[i], embeddings[j])\n",
        "            if distance < min_distance:\n",
        "                min_distance = distance\n",
        "                most_similar = (comments[i], comments[j])\n",
        "\n",
        "    # Write the most similar pair to file\n",
        "    with open(output_filename, 'w') as f:\n",
        "        f.write(most_similar[0] + '\\n')\n",
        "        f.write(most_similar[1] + '\\n')\n",
        "\n",
        "def A10(filename='/data/ticket-sales.db', output_filename='/data/ticket-sales-gold.txt', query=\"SELECT SUM(units * price) FROM tickets WHERE type = 'Gold'\"):\n",
        "    # Connect to the SQLite database\n",
        "    conn = sqlite3.connect(filename)\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    # Calculate the total sales for the \"Gold\" ticket type\n",
        "    cursor.execute(query)\n",
        "    total_sales = cursor.fetchone()[0]\n",
        "\n",
        "    # If there are no sales, set total_sales to 0\n",
        "    total_sales = total_sales if total_sales else 0\n",
        "\n",
        "    # Write the total sales to the file\n",
        "    with open(output_filename, 'w') as file:\n",
        "        file.write(str(total_sales))\n",
        "\n",
        "    # Close the database connection\n",
        "    conn.close()\n"
      ],
      "metadata": {
        "id": "mAbOE2Pob5iG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# tasksB.py"
      ],
      "metadata": {
        "id": "VnGPL5jjcFTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Phase B: LLM-based Automation Agent for DataWorks Solutions\n",
        "\n",
        "# B1 & B2: Security Checks\n",
        "import os\n",
        "\n",
        "def B12(filepath):\n",
        "    if filepath.startswith('/data'):\n",
        "        # raise PermissionError(\"Access outside /data is not allowed.\")\n",
        "        # print(\"Access outside /data is not allowed.\")\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "# B3: Fetch Data from an API\n",
        "def B3(url, save_path):\n",
        "    if not B12(save_path):\n",
        "        return None\n",
        "    import requests\n",
        "    response = requests.get(url)\n",
        "    with open(save_path, 'w') as file:\n",
        "        file.write(response.text)\n",
        "\n",
        "# B4: Clone a Git Repo and Make a Commit\n",
        "# def clone_git_repo(repo_url, commit_message):\n",
        "#     import subprocess\n",
        "#     subprocess.run([\"git\", \"clone\", repo_url, \"/data/repo\"])\n",
        "#     subprocess.run([\"git\", \"-C\", \"/data/repo\", \"commit\", \"-m\", commit_message])\n",
        "\n",
        "# B5: Run SQL Query\n",
        "def B5(db_path, query, output_filename):\n",
        "    if not B12(db_path):\n",
        "        return None\n",
        "    import sqlite3, duckdb\n",
        "    conn = sqlite3.connect(db_path) if db_path.endswith('.db') else duckdb.connect(db_path)\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(query)\n",
        "    result = cur.fetchall()\n",
        "    conn.close()\n",
        "    with open(output_filename, 'w') as file:\n",
        "        file.write(str(result))\n",
        "    return result\n",
        "\n",
        "# B6: Web Scraping\n",
        "def B6(url, output_filename):\n",
        "    import requests\n",
        "    result = requests.get(url).text\n",
        "    with open(output_filename, 'w') as file:\n",
        "        file.write(str(result))\n",
        "\n",
        "# B7: Image Processing\n",
        "def B7(image_path, output_path, resize=None):\n",
        "    from PIL import Image\n",
        "    if not B12(image_path):\n",
        "        return None\n",
        "    if not B12(output_path):\n",
        "        return None\n",
        "    img = Image.open(image_path)\n",
        "    if resize:\n",
        "        img = img.resize(resize)\n",
        "    img.save(output_path)\n",
        "\n",
        "# B8: Audio Transcription\n",
        "# def B8(audio_path):\n",
        "#     import openai\n",
        "#     if not B12(audio_path):\n",
        "#         return None\n",
        "#     with open(audio_path, 'rb') as audio_file:\n",
        "#         return openai.Audio.transcribe(\"whisper-1\", audio_file)\n",
        "\n",
        "# B9: Markdown to HTML Conversion\n",
        "def B9(md_path, output_path):\n",
        "    import markdown\n",
        "    if not B12(md_path):\n",
        "        return None\n",
        "    if not B12(output_path):\n",
        "        return None\n",
        "    with open(md_path, 'r') as file:\n",
        "        html = markdown.markdown(file.read())\n",
        "    with open(output_path, 'w') as file:\n",
        "        file.write(html)\n",
        "\n",
        "# B10: API Endpoint for CSV Filtering\n",
        "# from flask import Flask, request, jsonify\n",
        "# app = Flask(__name__)\n",
        "# @app.route('/filter_csv', methods=['POST'])\n",
        "# def filter_csv():\n",
        "#     import pandas as pd\n",
        "#     data = request.json\n",
        "#     csv_path, filter_column, filter_value = data['csv_path'], data['filter_column'], data['filter_value']\n",
        "#     B12(csv_path)\n",
        "#     df = pd.read_csv(csv_path)\n",
        "#     filtered = df[df[filter_column] == filter_value]\n",
        "#     return jsonify(filtered.to_dict(orient='records'))"
      ],
      "metadata": {
        "id": "mJsOonCZcHXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# datagen.py"
      ],
      "metadata": {
        "id": "4A7ElO6rcTwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DISCLAIMER: THIS SCRIPT WILL CHANGE BEFORE THE EVALUATION. TREAT THIS AS A GUIDE.\n",
        "\n",
        "# Usage: uv run datagen.py <email>\n",
        "\n",
        "# /// script\n",
        "# requires-python = \">=3.13\"\n",
        "# dependencies = [\n",
        "#     \"faker\",\n",
        "#     \"pillow\",\n",
        "# ]\n",
        "# ///\n",
        "\n",
        "import datetime\n",
        "import hashlib\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import sqlite3\n",
        "import time\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from faker import Faker\n",
        "\n",
        "config = {\"root\": \"/data\"}\n",
        "\n",
        "\n",
        "def num(str):\n",
        "    return int(hashlib.sha256(str.encode()).hexdigest(), 16) % (2**32)\n",
        "\n",
        "\n",
        "def write_file(path, content):\n",
        "    with open(os.path.join(config[\"root\"], path), \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(content)\n",
        "\n",
        "\n",
        "def get_markdown(email):\n",
        "    return f\"\"\"#Unformatted Markdown\n",
        "\n",
        "This  is a sample paragraph with extra  spaces and trailing whitespace.\n",
        "-   First item\n",
        "-    Second item\n",
        "+Third item\n",
        "    *    Fourth item\n",
        "\n",
        "```py\n",
        "print(\"{email}\")\n",
        "\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def a2_format_markdown():\n",
        "    \"\"\"Generate a poorly formatted markdown file at format.md.\n",
        "\n",
        "    This is a tricky file because formatting it _twice_ changes the format!\n",
        "    \"\"\"\n",
        "    write_file(\"format.md\", get_markdown(config[\"email\"]))\n",
        "\n",
        "\n",
        "def get_dates(email):\n",
        "    random.seed(f\"{email}:a3\", version=2)\n",
        "    start_date = datetime.datetime(2000, 1, 1)\n",
        "    end_date = datetime.datetime(2024, 12, 31)\n",
        "    formats = [\n",
        "        \"%Y-%m-%d\",  # 2024-03-14\n",
        "        \"%d-%b-%Y\",  # 14-Mar-2024\n",
        "        \"%b %d, %Y\",  # Mar 14, 2024\n",
        "        \"%Y/%m/%d %H:%M:%S\",  # 2024/03/14 15:30:45\n",
        "    ]\n",
        "    timestamps = random.sample(range(int(start_date.timestamp()), int(end_date.timestamp())), 1000)\n",
        "    return [\n",
        "        datetime.datetime.fromtimestamp(ts).strftime(random.choice(formats)) for ts in timestamps\n",
        "    ]\n",
        "\n",
        "\n",
        "def a3_dates():\n",
        "    \"\"\"Save 1,000 random non-unique dates between 2000-01-01 and 2024-12-31 at dates.txt\n",
        "\n",
        "    Generates dates in various unambiguous formats:\n",
        "    - ISO 8601: yyyy-mm-dd\n",
        "    - dd-MMM-yyyy\n",
        "    - MMM dd, yyyy\n",
        "    - yyyy/mm/dd HH:MM:SS\n",
        "    \"\"\"\n",
        "    dates = get_dates(config[\"email\"])\n",
        "    write_file(\"dates.txt\", \"\\n\".join(dates))\n",
        "\n",
        "\n",
        "def get_contacts(email):\n",
        "    fake = Faker()\n",
        "    fake.seed_instance(num(f\"{email}:a4\"))\n",
        "    return [\n",
        "        {\"first_name\": fake.first_name(), \"last_name\": fake.last_name(), \"email\": fake.email()}\n",
        "        for _ in range(100)\n",
        "    ]\n",
        "\n",
        "\n",
        "def a4_contacts():\n",
        "    \"\"\"Generate a JSON with 100 contacts with random first_name, last_name, and email\"\"\"\n",
        "    contacts = get_contacts(config[\"email\"])\n",
        "    write_file(\"contacts.json\", json.dumps(contacts))\n",
        "\n",
        "\n",
        "def get_logs(email):\n",
        "    files = []\n",
        "    random.seed(f\"{email}:a5\", version=2)\n",
        "    fake = Faker()\n",
        "    fake.seed_instance(num(f\"{email}:a5\"))\n",
        "    for i in range(50):\n",
        "        text = \"\\n\".join([fake.text() for _ in range(10)])\n",
        "        age = random.randint(1, 24 * 60 * 60 * 365)\n",
        "        files.append((age, text))\n",
        "    return files\n",
        "\n",
        "\n",
        "def a5_logs():\n",
        "    \"\"\"Generate 50 log files with 10 lines each of random content at logs/\"\"\"\n",
        "    email = config[\"email\"]\n",
        "    os.makedirs(os.path.join(config[\"root\"], \"logs\"), exist_ok=True)\n",
        "    now = time.time()\n",
        "    for i, (age, text) in enumerate(get_logs(email)):\n",
        "        write_file(f\"logs/log-{i}.log\", text)\n",
        "        os.utime(os.path.join(config[\"root\"], f\"logs/log-{i}.log\"), (now - age, now - age))\n",
        "\n",
        "\n",
        "def get_docs(email):\n",
        "    files = []\n",
        "    random.seed(f\"{email}:a6\", version=2)\n",
        "    fake = Faker()\n",
        "    fake.seed_instance(num(f\"{email}:a6\"))\n",
        "    for dir in fake.words(10):\n",
        "        for file in fake.words(10):\n",
        "            prefix = \"\\n\".join([fake.text() for _ in range(random.randint(0, 10))])\n",
        "            heading = f\"# {fake.sentence()}\"\n",
        "            suffix = \"\\n\".join([fake.text() for _ in range(random.randint(0, 10))])\n",
        "            text = \"\\n\".join([prefix, heading, suffix])\n",
        "            files.append((dir, file, text))\n",
        "    return files\n",
        "\n",
        "\n",
        "def a6_docs():\n",
        "    \"\"\"Generate 10 Markdown files each under 10 random subdirectories with random content.\"\"\"\n",
        "    email = config[\"email\"]\n",
        "    docs = get_docs(email)\n",
        "    os.makedirs(os.path.join(config[\"root\"], \"docs\"), exist_ok=True)\n",
        "    for dir, file, text in docs:\n",
        "        dirname = os.path.join(config[\"root\"], \"docs\", dir)\n",
        "        os.makedirs(dirname, exist_ok=True)\n",
        "        write_file(os.path.join(dirname, f\"{file}.md\"), text)\n",
        "\n",
        "\n",
        "def get_email(email):\n",
        "    fake = Faker()\n",
        "    fake.seed_instance(num(f\"{email}:a7\"))\n",
        "    email = {\n",
        "        \"recipient\": fake.email(),\n",
        "        \"from_name\": fake.name(),\n",
        "        \"from_email\": fake.email(),\n",
        "        \"date\": fake.date_time().strftime(\"%a, %d %b %Y %H:%M:%S +0000\"),\n",
        "        \"subject\": fake.sentence(),\n",
        "        \"recipient_name\": fake.name(),\n",
        "        \"cc_1_name\": fake.name(),\n",
        "        \"cc_1_email\": fake.email(),\n",
        "        \"cc_2_name\": fake.name(),\n",
        "        \"cc_2_email\": fake.email(),\n",
        "        \"cc_3_name\": fake.name(),\n",
        "        \"cc_3_email\": fake.email(),\n",
        "        \"body\": fake.text(),\n",
        "    }\n",
        "    return email\n",
        "\n",
        "\n",
        "def a7_email():\n",
        "    \"\"\"Generate an email file at email.txt\"\"\"\n",
        "    data = get_email(config[\"email\"])\n",
        "    write_file(\n",
        "        \"email.txt\",\n",
        "        f\"\"\"Delivered-To: {data[\"recipient\"]}\n",
        "MIME-Version: 1.0\n",
        "From: \"{data[\"from_name\"]}\" <{data[\"from_email\"]}>\n",
        "Date: {data[\"date\"]}\n",
        "Subject: {data[\"subject\"]}\n",
        "To: \"{data[\"recipient_name\"]}\" <{data[\"recipient\"]}>\n",
        "Cc: \"{data[\"cc_1_name\"]}\" <{data[\"cc_1_email\"]}>, \"{data[\"cc_2_name\"]}\" <{data[\"cc_2_email\"]}>, \"{data[\"cc_3_name\"]}\" <{data[\"cc_3_email\"]}>\n",
        "Content-Type: multipart/alternative; boundary=\"00000000000091a0ba062bcdefca\"\n",
        "\n",
        "--00000000000091a0ba062bcdefca\n",
        "Content-Type: text/plain; charset=\"UTF-8\"\n",
        "Content-Transfer-Encoding: quoted-printable\n",
        "\n",
        "{data[\"body\"]}\n",
        "\n",
        "--00000000000091a0ba062bcdefca--\n",
        "\"\"\",\n",
        "    )\n",
        "\n",
        "\n",
        "def get_credit_card(email):\n",
        "    fake = Faker()\n",
        "    fake.seed_instance(num(f\"{email}:a8\"))\n",
        "    return {\n",
        "        \"number\": fake.credit_card_number(),\n",
        "        \"expiry\": fake.credit_card_expire(),\n",
        "        \"security_code\": fake.credit_card_security_code(),\n",
        "        \"name\": fake.name().upper(),\n",
        "    }\n",
        "\n",
        "\n",
        "def a8_credit_card_image():\n",
        "    \"\"\"Generate a credit card image at credit_card.png that mimics a real credit card layout\"\"\"\n",
        "    data = get_credit_card(config[\"email\"])\n",
        "\n",
        "    # Create image with credit card proportions (3.375\" x 2.125\" at 300 DPI)\n",
        "    WIDTH, HEIGHT = 1012, 638\n",
        "    image = Image.new(\"RGB\", (WIDTH, HEIGHT), (25, 68, 141))  # Deep blue background\n",
        "    draw = ImageDraw.Draw(image)\n",
        "\n",
        "    # Use a larger font for credit card number, simplifying OCR\n",
        "    large_font = ImageFont.load_default()\n",
        "    large_font.size = 60\n",
        "\n",
        "    # Format credit card number with spaces\n",
        "    cc_number = \" \".join([data[\"number\"][i : i + 4] for i in range(0, 16, 4)])\n",
        "    # Position elements\n",
        "    draw.text((50, 250), cc_number, fill=(255, 255, 255), font=large_font)\n",
        "    draw.text((50, 400), \"VALID\\nTHRU\", fill=(255, 255, 255))\n",
        "    draw.text((50, 480), data[\"expiry\"], fill=(255, 255, 255))\n",
        "    draw.text((250, 480), data[\"security_code\"], fill=(255, 255, 255))\n",
        "    draw.text((50, 550), data[\"name\"], fill=(255, 255, 255))\n",
        "\n",
        "    image.save(os.path.join(config[\"root\"], \"credit_card.png\"))\n",
        "\n",
        "\n",
        "def get_comments(email):\n",
        "    fake = Faker()\n",
        "    fake.seed_instance(num(f\"{email}:a9\"))\n",
        "    return [fake.paragraph() for _ in range(100)]\n",
        "\n",
        "\n",
        "def a9_comments():\n",
        "    \"\"\"Generate a comments.txt file with 100 random comments\"\"\"\n",
        "    write_file(\"comments.txt\", \"\\n\".join(get_comments(config[\"email\"])))\n",
        "\n",
        "\n",
        "def get_tickets(email):\n",
        "    random.seed(f\"{email}:a10\", version=2)\n",
        "    ticket_types = [\"Gold\", \"Silver\", \"Bronze\"]\n",
        "    return [\n",
        "        (random.choice(ticket_types), random.randint(1, 10), round(random.uniform(50, 150), 2))\n",
        "        for _ in range(1000)\n",
        "    ]\n",
        "\n",
        "\n",
        "def a10_ticket_sales():\n",
        "    \"\"\"Generate ticket-sales.db with a tickets(type, units, price) table. 1 row per ticket\"\"\"\n",
        "    target = os.path.join(config[\"root\"], \"ticket-sales.db\")\n",
        "    if os.path.exists(target):\n",
        "        os.remove(target)\n",
        "    conn = sqlite3.connect(target)\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute(\n",
        "        \"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS tickets (\n",
        "            type TEXT NOT NULL,\n",
        "            units INTEGER NOT NULL,\n",
        "            price DECIMAL(10,2) NOT NULL\n",
        "        )\n",
        "    \"\"\"\n",
        "    )\n",
        "    cursor.executemany(\"INSERT INTO tickets VALUES (?, ?, ?)\", get_tickets(config[\"email\"]))\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"email\")\n",
        "    parser.add_argument(\"--root\", default=\"/data\")\n",
        "    args = parser.parse_args()\n",
        "    config[\"email\"] = args.email\n",
        "    config[\"root\"] = os.path.abspath(args.root)\n",
        "\n",
        "    os.makedirs(config[\"root\"], exist_ok=True)\n",
        "\n",
        "    print(\"DISCLAIMER: THIS SCRIPT WILL CHANGE BEFORE THE EVALUATION. TREAT THIS AS A GUIDE.\")\n",
        "    print(\"Files created at\", config[\"root\"])\n",
        "\n",
        "    a2_format_markdown()\n",
        "    a3_dates()\n",
        "    a4_contacts()\n",
        "    a5_logs()\n",
        "    a6_docs()\n",
        "    a7_email()\n",
        "    a8_credit_card_image()\n",
        "    a9_comments()\n",
        "    a10_ticket_sales()\n",
        "\n",
        "# DISCLAIMER: THIS SCRIPT WILL CHANGE BEFORE THE EVALUATION. TREAT THIS AS A GUIDE."
      ],
      "metadata": {
        "id": "kSvxJdgKcV4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# evaluate.py"
      ],
      "metadata": {
        "id": "XGWnX_ZCcYvk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# /// script\n",
        "# requires-python = \">=3.13\"\n",
        "# dependencies = [\n",
        "#     \"faker\",\n",
        "#     \"httpx\",\n",
        "#     \"numpy\",\n",
        "#     \"pillow\",\n",
        "#     \"python-dateutil\",\n",
        "# ]\n",
        "# ///\n",
        "import hashlib\n",
        "import httpx\n",
        "import json\n",
        "import logging\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import subprocess\n",
        "from dateutil.parser import parse\n",
        "from datagen import (\n",
        "    get_markdown,\n",
        "    get_dates,\n",
        "    get_contacts,\n",
        "    get_logs,\n",
        "    get_docs,\n",
        "    get_email,\n",
        "    get_credit_card,\n",
        "    get_comments,\n",
        "    get_tickets,\n",
        ")\n",
        "\n",
        "\n",
        "openai_api_base = os.getenv(\"OPENAI_API_BASE\", \"https://aiproxy.sanand.workers.dev/openai/v1\")\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "\n",
        "def num(str):\n",
        "    return int(hashlib.sha256(str.encode()).hexdigest(), 16) % (2**32)\n",
        "\n",
        "\n",
        "def mismatch(msg, expected, result):\n",
        "    logging.error(f\"🔴 {msg}\\n⚠️ EXPECTED:\\n{expected}\\n⚠️ RESULT:\\n{result}\")\n",
        "    return False\n",
        "\n",
        "\n",
        "async def run(task: str):\n",
        "    async with httpx.AsyncClient(timeout=30) as client:\n",
        "        logging.warning(f\"🟡 Running task: {task.strip()}\")\n",
        "        response = await client.post(\"http://localhost:8000/run\", params={\"task\": task})\n",
        "        try:\n",
        "            response_text = json.dumps(response.json(), indent=2)\n",
        "        except json.JSONDecodeError:\n",
        "            response_text = response.text\n",
        "        if response.status_code < 400:\n",
        "            logging.info(f\"🟢 HTTP {response.status_code} {response_text}\")\n",
        "        else:\n",
        "            logging.error(f\"🔴 HTTP {response.status_code} {response_text}\")\n",
        "        return response.status_code, response_text\n",
        "\n",
        "\n",
        "async def read(path: str):\n",
        "    async with httpx.AsyncClient(timeout=30) as client:\n",
        "        response = await client.get(f\"http://localhost:8000/read?path={path}\")\n",
        "        if response.status_code != 200:\n",
        "            raise Exception(f\"Cannot read {path}\")\n",
        "        return response.text\n",
        "\n",
        "\n",
        "async def a1(email: str, **kwargs):\n",
        "    await run(\n",
        "        f\"\"\"\n",
        "Install `uv` (if required) and run the script `https://raw.githubusercontent.com/sanand0/tools-in-data-science-public/tds-2025-01/datagen.py`\n",
        "with `{email}` as the only argument\n",
        "\"\"\"\n",
        "    )\n",
        "    return email in await read(\"/data/format.md\")\n",
        "\n",
        "\n",
        "async def a2(email: str, file: str = \"/data/format.md\", **kwargs):\n",
        "    original = get_markdown(email)\n",
        "    expected = subprocess.run(\n",
        "        [\"npx\", \"prettier@3.4.2\", \"--stdin-filepath\", file],\n",
        "        input=original,\n",
        "        capture_output=True,\n",
        "        text=True,\n",
        "        check=True,\n",
        "        # Ensure npx is picked up from the PATH on Windows\n",
        "        shell=True,\n",
        "    ).stdout\n",
        "    result = await run(\n",
        "        f\"\"\"\n",
        "Format the contents of `{file}` using `prettier@3.4.2`, updating the file in-place\n",
        "\"\"\"\n",
        "    )\n",
        "    result = await read(file)\n",
        "    if result != expected:\n",
        "        return mismatch(file, expected, result)\n",
        "    return True\n",
        "\n",
        "\n",
        "async def a3(email, **kwargs):\n",
        "    dates = get_dates(email)\n",
        "    await run(\n",
        "        \"The file `/data/dates.txt` contains a list of dates, one per line. Count the number of Wednesdays in the list, and write just the number to `/data/dates-wednesdays.txt`\"\n",
        "    )\n",
        "    result = await read(\"/data/dates-wednesdays.txt\")\n",
        "    expected = sum(1 for date in dates if parse(date).weekday() == 2)\n",
        "    if result.strip() != str(expected):\n",
        "        return mismatch(\"/data/dates-wednesdays.txt\", expected, result)\n",
        "    return True\n",
        "\n",
        "\n",
        "async def a4(email, **kwargs):\n",
        "    contacts = get_contacts(email)\n",
        "    contacts.sort(key=lambda c: (c[\"last_name\"], c[\"first_name\"]))\n",
        "    await run(\n",
        "        \"Sort the array of contacts in `/data/contacts.json` by `last_name`, then `first_name`, and write the result to `/data/contacts-sorted.json`\"\n",
        "    )\n",
        "    result = await read(\"/data/contacts-sorted.json\")\n",
        "    try:\n",
        "        result = json.loads(result)\n",
        "    except json.JSONDecodeError:\n",
        "        logging.error(\"🔴 /data/contacts-sorted.json was not valid JSON\")\n",
        "        return False\n",
        "    if json.dumps(result, sort_keys=True) != json.dumps(contacts, sort_keys=True):\n",
        "        return mismatch(\"/data/contacts-sorted.json\", contacts, result)\n",
        "    return True\n",
        "\n",
        "\n",
        "async def a5(email, **kwargs):\n",
        "    files = get_logs(email)\n",
        "    files.sort(key=lambda f: f[0])\n",
        "    expected = \"\".join([f[1].split(\"\\n\")[0] + \"\\n\" for f in files[:10]])\n",
        "    await run(\n",
        "        \"Write the first line of the 10 most recent `.log` file in `/data/logs/` to `/data/logs-recent.txt`, most recent first\"\n",
        "    )\n",
        "    result = await read(\"/data/logs-recent.txt\")\n",
        "    if result.strip() != expected.strip():\n",
        "        return mismatch(\"/data/logs-recent.txt\", expected, result)\n",
        "    return True\n",
        "\n",
        "\n",
        "# TODO: Verify after datagen\n",
        "async def a6(email, **kwargs):\n",
        "    docs = get_docs(email)\n",
        "    await run(\n",
        "        \"\"\"Find all Markdown (`.md`) files in `/data/docs/`.\n",
        "For each file, extract the first occurrance of each H1 (i.e. a line starting with `# `).\n",
        "Create an index file `/data/docs/index.json` that maps each filename (without the `/data/docs/` prefix) to its title\n",
        "(e.g. `{\"README.md\": \"Home\", \"path/to/large-language-models.md\": \"Large Language Models\", ...}`)\"\"\"\n",
        "    )\n",
        "    expected = {}\n",
        "    for dir, file, text in docs:\n",
        "        # get the first line starting with #\n",
        "        for line in text.split(\"\\n\"):\n",
        "            if line.startswith(\"# \"):\n",
        "                title = line[2:].strip()\n",
        "                break\n",
        "        expected[f\"{dir}/{file}.md\"] = title\n",
        "    result = await read(\"/data/docs/index.json\")\n",
        "    try:\n",
        "        result = json.loads(result)\n",
        "    except json.JSONDecodeError:\n",
        "        logging.error(\"🔴 /data/docs/index.json was not valid JSON\")\n",
        "        return False\n",
        "    if json.dumps(result, sort_keys=True) != json.dumps(expected, sort_keys=True):\n",
        "        return mismatch(\"/data/docs/index.json\", expected, result)\n",
        "    return True\n",
        "\n",
        "\n",
        "async def a7(email, **kwargs):\n",
        "    expected = get_email(email)[\"from_email\"]\n",
        "    await run(\n",
        "        \"`/data/email.txt` contains an email message. Pass the content to an LLM with instructions to extract the sender's email address, and write just the email address to `/data/email-sender.txt`\"\n",
        "    )\n",
        "    result = await read(\"/data/email-sender.txt\")\n",
        "    if result != expected:\n",
        "        return mismatch(\"/data/email-sender.txt\", expected, result)\n",
        "    return True\n",
        "\n",
        "\n",
        "async def a8(email, **kwargs):\n",
        "    data = get_credit_card(email)\n",
        "    await run(\n",
        "        \"`/data/credit_card.png` contains a credit card number. Pass the image to an LLM, have it extract the card number, and write it without spaces to `/data/credit-card.txt`\"\n",
        "    )\n",
        "    result = await read(\"/data/credit-card.txt\")\n",
        "    if re.sub(r\"\\D\", \"\", result) != re.sub(r\"\\D\", \"\", data[\"number\"]):\n",
        "        return mismatch(\"/data/credit-card.txt\", data[\"number\"], result)\n",
        "    return True\n",
        "\n",
        "\n",
        "async def a9(email, **kwargs):\n",
        "    data = get_comments(email)\n",
        "    async with httpx.AsyncClient(timeout=30) as client:\n",
        "        response = await client.post(\n",
        "            f\"{openai_api_base}/embeddings\",\n",
        "            headers={\"Authorization\": f\"Bearer {openai_api_key}\"},\n",
        "            json={\"model\": \"text-embedding-3-small\", \"input\": data},\n",
        "        )\n",
        "    embeddings = np.array([emb[\"embedding\"] for emb in response.json()[\"data\"]])\n",
        "    similarity = np.dot(embeddings, embeddings.T)\n",
        "    # Create mask to ignore diagonal (self-similarity)\n",
        "    np.fill_diagonal(similarity, -np.inf)\n",
        "    # Get indices of maximum similarity\n",
        "    i, j = np.unravel_index(similarity.argmax(), similarity.shape)\n",
        "    expected = \"\\n\".join(sorted([data[i], data[j]]))\n",
        "    await run(\n",
        "        \"`/data/comments.txt` contains a list of comments, one per line. Using embeddings, find the most similar pair of comments and write them to `/data/comments-similar.txt`, one per line\"\n",
        "    )\n",
        "    result = await read(\"/data/comments-similar.txt\")\n",
        "    sorted_result = \"\\n\".join(sorted([line for line in result.split(\"\\n\") if line.strip()]))\n",
        "    if sorted_result != expected:\n",
        "        return mismatch(\"/data/comments-similar.txt\", expected, result)\n",
        "    return True\n",
        "\n",
        "\n",
        "async def a10(email, **kwargs):\n",
        "    data = get_tickets(email)\n",
        "    await run(\n",
        "        'The SQLite database file `/data/ticket-sales.db` has a `tickets` with columns `type`, `units`, and `price`. Each row is a customer bid for a concert ticket. What is the total sales of all the items in the \"Gold\" ticket type? Write the number in `/data/ticket-sales-gold.txt`'\n",
        "    )\n",
        "    result = await read(\"/data/ticket-sales-gold.txt\")\n",
        "    expected = sum(row[1] * row[2] for row in data if row[0].lower() == \"gold\")\n",
        "    try:\n",
        "        result = float(result)\n",
        "    except ValueError:\n",
        "        logging.error(f\"🔴 /data/ticket-sales-gold.txt was {result}, not a valid number\")\n",
        "        return False\n",
        "    if abs(result - expected) > 0.1:\n",
        "        return mismatch(\"/data/ticket-sales-gold.txt\", expected, result)\n",
        "    return True\n",
        "\n",
        "\n",
        "async def main(email: str):\n",
        "    score, total = 0, 0\n",
        "    for task in [a1, a2, a3, a4, a5, a6, a7, a8, a9, a10]:\n",
        "        total += 1\n",
        "        try:\n",
        "            success = await task(email=email)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"🔴 {task.__name__.upper()} failed: {e}\")\n",
        "            success = False\n",
        "        if success:\n",
        "            logging.info(f\"✅ {task.__name__.upper()} PASSED\")\n",
        "        else:\n",
        "            logging.error(f\"❌ {task.__name__.upper()} FAILED\")\n",
        "        score += 1 if success else 0\n",
        "    logging.info(f\"🎯 Score: {score} / {total}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import asyncio\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"Evaluate tasks with configurable logging\")\n",
        "    parser.add_argument(\"--email\", default=\"user@example.com\", help=\"Set the email address\")\n",
        "    levels = [\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"]\n",
        "    parser.add_argument(\"--log-level\", default=\"INFO\", choices=levels, help=\"Set logging level\")\n",
        "    args = parser.parse_args()\n",
        "    logging.basicConfig(level=args.log_level, format=\"%(message)s\\n\")\n",
        "    asyncio.run(main(args.email))"
      ],
      "metadata": {
        "id": "waFjyHVXcadI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dockerfile"
      ],
      "metadata": {
        "id": "NFHnpCeHcypg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FROM python:3.12-slim-bookworm\n",
        "\n",
        "# Install dependencies\n",
        "RUN apt-get update && apt-get install -y --no-install-recommends curl ca-certificates\n",
        "\n",
        "# Download and install uv\n",
        "ADD https://astral.sh/uv/install.sh /uv-installer.sh\n",
        "RUN sh /uv-installer.sh && rm /uv-installer.sh\n",
        "\n",
        "# Install FastAPI and Uvicorn\n",
        "RUN pip install fastapi uvicorn\n",
        "\n",
        "# Ensure the installed binary is on the `PATH`\n",
        "ENV PATH=\"/root/.local/bin:$PATH\"\n",
        "\n",
        "# Set up the application directory\n",
        "WORKDIR /app\n",
        "\n",
        "# Copy application files\n",
        "COPY app.py /app\n",
        "\n",
        "# Explicitly set the correct binary path and use `sh -c`\n",
        "CMD [\"/root/.local/bin/uv\", \"run\", \"app.py\"]"
      ],
      "metadata": {
        "id": "jf75gf77c0yb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "hd9ky80Kidem"
      }
    }
  ]
}